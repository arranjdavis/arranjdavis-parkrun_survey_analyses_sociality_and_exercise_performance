PCA.mod.final <-principal(cor.dat, nfactors = 1, rotate = "oblimin", scores = TRUE)
PCA.mod.final
#Cronbach's alpha
psych::alpha(cor.dat, warnings = FALSE)
#add scores to the main dataset
parkrun_community_factor <- cbind(mydata, PCA.mod.final$scores)
mydata$parkrun_community_factor = parkrun_community_factor$PC1
################################################################################################################################################
### GET SURVEY RESPONSE TIMES ###
#combine columns for date and time survey was sent
mydata <- transform(mydata, newcol=paste(date_ymd, survey_send, sep="  "))
#rename column
names(mydata)[names(mydata) == 'newcol'] <- 'survey_received'
#convert character string to time
Sys.setenv(TZ='Europe/London') #updates system time zone (perhaps unnecessary)
mydata$survey_received <- as.POSIXct(strptime(mydata$survey_received, "%Y-%m-%d %H:%M:%S"))
mydata$survey_end <- as.POSIXct(strptime(mydata$survey_end, "%Y-%m-%d %H:%M:%S"))
mydata$survey_begin <- as.POSIXct(strptime(mydata$survey_begin, "%Y-%m-%d %H:%M:%S"))
### TIME TO COMPLETE SURVEY (SURVEY RECIEVED TO SURVEY SUBMITTED) ###
#in minutes
mydata$time_to_respond = as.numeric(mydata$survey_end - mydata$survey_received)
mean(mydata$time_to_respond)
median(mydata$time_to_respond)
sd(mydata$time_to_respond)
range(mydata$time_to_respond)
skewness(mydata$time_to_respond)
#in hours
mydata$time_to_respond_hr = mydata$time_to_respond / 60
mean(mydata$time_to_respond_hr)
median(mydata$time_to_respond_hr)
sd(mydata$time_to_respond_hr)
range(mydata$time_to_respond_hr)
skewness(mydata$time_to_respond_hr)
#create a histogram of the time taken to submit the survey
time_to_submit <- ggplot(mydata, aes(x=time_to_respond_hr)) +
geom_histogram(bins = 167, alpha=0.5, position="identity", colour = "black") +
theme(plot.background = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),
panel.background = element_blank(), axis.line.x = element_line(color="black", size = 0.2), axis.line.y = element_line(color="black", size = 0.2),
legend.key = element_blank(), text = element_text(size = 20, family = "sans"), axis.text.y = element_blank()) +
xlab("Time taken to submit survey (hr)") +
ylab("Count") +
theme(axis.title.x = element_text(family="Arial", face="bold", colour="black", size=rel(1), margin = margin(t = 10, r = 0, b = 0, l = 0))) +
theme(axis.title.y = element_text(family="Arial", face="bold", colour="black", size=rel(1))) +
theme(axis.text.x = element_text(family="Arial", colour="black", size=rel(1))) +
theme(axis.text.y = element_text(family="Arial", colour="black", size=rel(1))) +
scale_x_continuous(breaks = c(seq(0,168,24))) +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),panel.background = element_blank())
ggsave("../outputs/time_to_submit_hist.jpg", time_to_submit, width = 10, height = 10)
#give percentage of participants that took less than one hour to respond
(1 - (table(mydata$time_to_respond < 60)[1] / sum(table(mydata$time_to_respond < 60)))) * 100
#total number of participants that took less than one hour to respond
nrow(mydata) - table(mydata$time_to_respond < 60)[1]
#percentage
(nrow(mydata) - table(mydata$time_to_respond < 60)[1]) / nrow(mydata)
#give how many people responded within two hours
nrow(subset(mydata, time_to_respond < (2*60)))
#percentage
(nrow(mydata) - table(mydata$time_to_respond < (2*60))[1]) / nrow(mydata)
#give how many people responded within one day
nrow(subset(mydata, time_to_respond < (24*60)))
#percentage
(nrow(mydata) - table(mydata$time_to_respond < (24*60))[1]) / nrow(mydata)
#give how many people responded within two days
nrow(subset(mydata, time_to_respond < (48*60)))
#percentage
(nrow(mydata) - table(mydata$time_to_respond < (48*60))[1]) / nrow(mydata)
#give how many people responded within four days
nrow(subset(mydata, time_to_respond < (96*60)))
#percentage
(nrow(mydata) - table(mydata$time_to_respond < (96*60))[1]) / nrow(mydata)
### TIME TAKEN TO FILL OUT SURVEY (SURVEY BEGIN TO SURVEY END) ###
mydata$time_to_fill_out = as.numeric((mydata$survey_end- mydata$survey_begin) / 60)
#in minutes
mean(mydata$time_to_fill_out)
sd(mydata$time_to_fill_out)
range(mydata$time_to_fill_out)
median(mydata$time_to_fill_out)
#get percent of responses that took less than 5 minutes (survey was designed to take 5 minutes)
(1 - (table(mydata$time_to_fill_out < 5)[1] / sum(table(mydata$time_to_fill_out < 10)))) * 100
#less than 10 minutes
(1 - (table(mydata$time_to_fill_out < 10)[1] / sum(table(mydata$time_to_fill_out < 5)))) * 100
#get the percentage that took more than half an hour
(table(mydata$time_to_fill_out < 30)[1] / sum(table(mydata$time_to_fill_out < 30))) * 100
#remove big outliers
table(mydata$time_to_fill_out)
q = subset(mydata, time_to_fill_out < 30)
#create a histogram
time_to_complete <- ggplot(q, aes(x=time_to_fill_out)) +
geom_histogram(bins = 30, alpha=0.5, position="identity", colour = "black") +
theme(plot.background = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),
panel.background = element_blank(), axis.line.x = element_line(color="black", size = 0.2), axis.line.y = element_line(color="black", size = 0.2),
legend.key = element_blank(), text = element_text(size = 20, family = "sans"), axis.text.y = element_blank()) +
xlab("Time taken to complete survey (min)") +
ylab("Count") +
theme(axis.title.x = element_text(family="Arial", face="bold", colour="black", size=rel(1), margin = margin(t = 10, r = 0, b = 0, l = 0))) +
theme(axis.title.y = element_text(family="Arial", face="bold", colour="black", size=rel(1))) +
theme(axis.text.x = element_text(family="Arial", colour="black", size=rel(1))) +
theme(axis.text.y = element_text(family="Arial", colour="black", size=rel(1))) +
scale_x_continuous(breaks = c(seq(0,35,5))) +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),panel.background = element_blank())
ggsave("../outputs/time_to_complete_hist.jpg", time_to_complete, width = 10, height = 10)
################################################################################################################################################
### GET THE NUMBER OF RESPONSES PER PARTICIPANT ###
total_survey_responses = mydata %>%
#sumarise unique locations attended by each parkrunner
group_by(Athlete_ID) %>%
summarize(n = n_distinct(survey_end))
table(total_survey_responses$n)
#make a histogram of this
responses_per_participant <- ggplot(total_survey_responses, aes(x=n)) +
geom_histogram(bins = 17, alpha=0.5, position="identity", colour = "black") +
theme(plot.background = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),
panel.background = element_blank(), axis.line.x = element_line(color="black", size = 0.2), axis.line.y = element_line(color="black", size = 0.2),
legend.key = element_blank(), text = element_text(size = 20, family = "sans"), axis.text.y = element_blank()) +
xlab("Total number of survey responses") +
ylab("Count") +
theme(axis.title.x = element_text(family="Arial", face="bold", colour="black", size=rel(1), margin = margin(t = 10, r = 0, b = 0, l = 0))) +
theme(axis.title.y = element_text(family="Arial", face="bold", colour="black", size=rel(1))) +
theme(axis.text.x = element_text(family="Arial", colour="black", size=rel(1))) +
theme(axis.text.y = element_text(family="Arial", colour="black", size=rel(1))) +
scale_x_continuous(breaks = c(seq(0,17,1))) +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),panel.background = element_blank())
ggsave("../outputs/responses_per_participant_hist.jpg", responses_per_participant, width = 10, height = 10)
#descriptives for number of responses per participant
mean(total_survey_responses$n)
sd(total_survey_responses$n)
range(total_survey_responses$n)
median(total_survey_responses$n)
skewness(total_survey_responses$n)
################################################################################################################################################
### GENDER DESCRIPTIVES ###
#subset one row per participant
one_run_per <- mydata %>% group_by(Athlete_ID) %>% sample_n(1)
#percent of female participants
table(one_run_per$gender)[2] / (sum(table(one_run_per$gender)))
#percentage of total responses filled out by females
table(mydata$gender)[2] / (sum(table(mydata$gender)))
### 5 KM RUN TIME DESCRIPTIVES ###
#get run time skew, mean, SD, and range
skewness(mydata$time)
mean(mydata$time)
sd(mydata$time)
range(mydata$time)
#make a histogram
run_time_hist = ggplot(mydata, aes(x=time)) +
geom_histogram(bins = 35, alpha=0.5, position="identity", colour = "black") +
theme(plot.background = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),
panel.background = element_blank(), axis.line.x = element_line(color="black", size = 0.2), axis.line.y = element_line(color="black", size = 0.2),
legend.key = element_blank(), text = element_text(size = 20, family = "sans"), axis.text.y = element_blank()) +
xlab("5 km run time (min)") +
ylab("Count") +
theme(axis.title.x = element_text(family="Arial", face="bold", colour="black", size=rel(1), margin = margin(t = 10, r = 0, b = 0, l = 0))) +
theme(axis.title.y = element_text(family="Arial", face="bold", colour="black", size=rel(1))) +
theme(axis.text.x = element_text(family="Arial", colour="black", size=rel(1))) +
theme(axis.text.y = element_text(family="Arial", colour="black", size=rel(1))) +
scale_x_continuous(breaks = c(seq(15,60,5))) +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),panel.background = element_blank())
ggsave("../outputs/run_time_hist.jpg", run_time_hist, width = 10, height = 10)
################################################################################################################################################
### RPE AND FATIGUE RELATIONSHIP ###
#correlation between RPE and fatigue
cor(mydata$exertion, mydata$fatigued)
#mean, sd, and range ofg RPE variable
mean(mydata$exertion + 5)
sd(mydata$exertion + 5)
range(mydata$exertion + 5)
#make a histogram of this
rpe_responses <- ggplot(mydata, aes(x=exertion + 5)) +
geom_histogram(bins = 15, alpha=0.5, position="identity", colour = "black") +
theme(plot.background = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),
panel.background = element_blank(), axis.line.x = element_line(color="black", size = 0.2), axis.line.y = element_line(color="black", size = 0.2),
legend.key = element_blank(), text = element_text(size = 20, family = "sans"), axis.text.y = element_blank()) +
xlab("Rate of perceived exertion (RPE)") +
ylab("Count") +
theme(axis.title.x = element_text(family="Arial", face="bold", colour="black", size=rel(1), margin = margin(t = 10, r = 0, b = 0, l = 0))) +
theme(axis.title.y = element_text(family="Arial", face="bold", colour="black", size=rel(1))) +
theme(axis.text.x = element_text(family="Arial", colour="black", size=rel(1))) +
theme(axis.text.y = element_text(family="Arial", colour="black", size=rel(1))) +
scale_x_continuous(breaks = c(seq(6,20,1))) +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_blank(),panel.background = element_blank())
ggsave("../outputs/rpe_responses_hist.jpg", rpe_responses, width = 10, height = 10)
################################################################################################################################################
### DESCRIPTIVE STATISTICS TABLE FOR NON-SOCIAL VARIABLES ###
#create the table
library(officer)
library(flextable)
library(magrittr)
#create a data frame of the variables of interest
non_social_dat = as.data.frame(mydata[, c("fatigued",
"pain_of_fatigue",
"enjoyment",
"how_energising")])
#get the total number of responses for the 'pain_of_fatigue' variable
sum(table(mydata$pain_of_fatigue, useNA = "no"))
#get the percentage of times that participants answered the 'fatigued' question with a 5 or more (leading them to be asked the 'pain_of_fatigue' question)
(sum(table(mydata$pain_of_fatigue, useNA = "no")) / nrow(mydata)) * 100
#summarise the main variables
non_social_dat_mean_summary = as.data.frame(t(non_social_dat %>% summarise_all(funs(mean), na.rm = TRUE)))
non_social_dat_sd_summary = as.data.frame(t(non_social_dat %>% summarise_all(funs(sd), na.rm = TRUE)))
non_social_dat_min_summary = as.data.frame(t(non_social_dat %>% summarise_all(funs(min), na.rm = TRUE)))
non_social_dat_max_summary = as.data.frame(t(non_social_dat %>% summarise_all(funs(max), na.rm = TRUE)))
#create an empty data frame with each question type
non_social_dat_summary = data.frame(matrix(NA, nrow = 4))
non_social_dat_summary$Question =  c("How physically fatigued did you feel during your run today?",
"How physically painful did this fatigue feel?",
"How much did you enjoy your run today?",
"How energising did it feel to be with the other parkrunners today?")
#add each summary to the dataset, and drop the original NA column
non_social_dat_summary$Mean = round(non_social_dat_mean_summary$V1, 3)
non_social_dat_summary$SD = round(non_social_dat_sd_summary$V1, 3)
non_social_dat_summary$Minimum = non_social_dat_min_summary$V1
non_social_dat_summary$Maximum = non_social_dat_max_summary$V1
#create a range variable
non_social_dat_summary$Range = paste(non_social_dat_summary$Minimum, non_social_dat_summary$Maximum, sep = " - ")
#drop unnecessary variables
non_social_dat_summary = non_social_dat_summary[ , c("Question","Mean", "SD", "Range")]
#create flextable object
ft <- flextable(data = non_social_dat_summary) %>% theme_zebra %>% autofit
#see flextable in RStudio viewer
ft
#create a temporary file
tmp <- tempfile(fileext = "wellbeing_t4_summary.docx")
#create a 'docx' file
read_docx() %>%
body_add_flextable(ft) %>%
print(target = tmp)
#open Word document
browseURL(tmp)
#get the count of runs where participants reported speeding up or slowing down to run with their running partners
table(mydata$slowed_spedup_natural)
#percentage slowed down to run with their running partner
(table(mydata$slowed_spedup_natural)[1] / sum(table(mydata$slowed_spedup_natural))) * 100
#percentage sped up to run with their running partner
(table(mydata$slowed_spedup_natural)[2] / sum(table(mydata$slowed_spedup_natural))) * 100
#percentage who ran at natural pace with their running partner
(table(mydata$slowed_spedup_natural)[3] / sum(table(mydata$slowed_spedup_natural))) * 100
#percentage who ran on their own
(table(mydata$slowed_spedup_natural)[4] / sum(table(mydata$slowed_spedup_natural))) * 100
################################################################################################################################################
### DESCRIPTIVE STATISTICS OF SURVEY QUESTIONS ###
#motivations for attending parkrun (1 = "to improve my ranking"; 2 = "to improve my time; 3 = "to run together with other people")
table(mydata$ranking_time_people)
#in percentages
(table(mydata$ranking_time_people)[1] / sum(table(mydata$ranking_time_people))) * 100
(table(mydata$ranking_time_people)[2] / sum(table(mydata$ranking_time_people))) * 100
(table(mydata$ranking_time_people)[3] / sum(table(mydata$ranking_time_people))) * 100
#who people ran with (1 = "on my own", 2 = "alongside one or more acquaintances", 3 = "alongside one or more friends/family members", 4 = "alongside a mix of acquaintances and friends/family members")
table(mydata$run_description)
#in percentages
(table(mydata$run_description)[1] / sum(table(mydata$run_description))) * 100
(table(mydata$run_description)[2] / sum(table(mydata$run_description))) * 100
(table(mydata$run_description)[3] / sum(table(mydata$run_description))) * 100
(table(mydata$run_description)[4] / sum(table(mydata$run_description))) * 100
#who participants came or met up with (1 = "nobody else"; 2 = "one or more acquaintances"; 3 = "one or more friends/family members or a mix of acquaintances and friends/family members")
table(mydata$came_with)
table(mydata$came_with2)
#in percentages
(table(mydata$came_with)[1] / sum(table(mydata$came_with))) * 100
(table(mydata$came_with)[2] / sum(table(mydata$came_with))) * 100
(table(mydata$came_with)[3] / sum(table(mydata$came_with))) * 100
#percentages for the recoded, binary variable
(table(mydata$came_with2)[1] / sum(table(mydata$came_with2))) * 100
(table(mydata$came_with2)[2] / sum(table(mydata$came_with2))) * 100
#pre-run sociality
table(mydata$just_before)
table(mydata$just_before2)
#in percentages
(table(mydata$just_before)[1] / sum(table(mydata$just_before))) * 100
(table(mydata$just_before)[2] / sum(table(mydata$just_before))) * 100
(table(mydata$just_before)[3] / sum(table(mydata$just_before))) * 100
#percentages for the recoded, binary variable
(table(mydata$just_before2)[1] / sum(table(mydata$just_before2))) * 100
(table(mydata$just_before2)[2] / sum(table(mydata$just_before2))) * 100
################################################################################################################################################
### EFFECT OF RESPONSE LAG ON OUTCOME-RELEVANT VARIABLES ###
library(lmerTest)
library(MuMIn)
#test whether response lag times predict responses to any of the survey questions
time.fatigue = lmer(fatigued ~ 1 + log(time_to_respond) + (1 + log(time_to_respond) | Athlete_ID), data = mydata)
summary(time.fatigue)
r.squaredGLMM(time.fatigue)
time.energy = lmer(how_energising ~ 1 + log(time_to_respond) + (1 + log(time_to_respond) | Athlete_ID), data = mydata)
summary(time.energy)
r.squaredGLMM(time.energy)
time.enjoyment = lmer(enjoyment ~ 1 + log(time_to_respond) + (1 + log(time_to_respond) | Athlete_ID), data = mydata)
summary(time.enjoyment)
r.squaredGLMM(time.enjoyment)
time.pcc = lmer(parkrun_community_factor ~ 1 + log(time_to_respond) + (1 + log(time_to_respond) | Athlete_ID), data = mydata)
summary(time.pcc)
r.squaredGLMM(time.pcc)
################################################################################################################################################
### DESCRIPTIVES FOR HIGH AND LOW RESPONDERS ###
#create a variable that is the total number of survey responses for each participant
mydata$total_responses = ave(mydata$Survey_number, mydata[,c("Athlete_ID")], FUN=length)
#get the mean and standard deviation
mean(mydata$total_responses)
#get the number of responses by participant needed to make up at least 50% of the dataset
for (i in seq(min(mydata$total_responses), max(mydata$total_responses))){
sub_dat = subset(mydata, total_responses >= i)
percent_total = (nrow(sub_dat)/ nrow(mydata)) * 100
print(paste("Minimum survey responses per participant:", i, ", Total responses in resulting dataset:", nrow(sub_dat),
", Number of participants in resulting dataset", length(unique(sub_dat$Athlete_ID)), ", Percent of total responses:", percent_total))
}
### ### ###
#participants who responded nine or more times made up over half of the dataset
high_responders = subset(mydata, total_responses >= 9)
low_responders = subset(mydata, total_responses < 9)
#count and percentage of participants and responses in each category
nrow(high_responders)
(nrow(high_responders)/nrow(mydata)) *100
length(unique(high_responders$Athlete_ID))
(length(unique(high_responders$Athlete_ID)) / length(unique(mydata$Athlete_ID))) * 100
nrow(low_responders)
(nrow(low_responders)/nrow(mydata)) *100
length(unique(low_responders$Athlete_ID))
(length(unique(low_responders$Athlete_ID)) / length(unique(mydata$Athlete_ID))) * 100
### ### ###
#get the age make-up of high responders
age_data_h <- high_responders %>%
group_by(Athlete_ID) %>%
summarise(n = mean(age_integer))
#mean age for high responders
mean(age_data_h$n)
sd(age_data_h$n)
#get the age make-up of low responders
age_data_l <- low_responders %>%
group_by(Athlete_ID) %>%
summarise(n = mean(age_integer))
#mean age for high responders
mean(age_data_l$n)
sd(age_data_l$n)
#compare the distributions
t.test(age_data_h$n, age_data_l$n)
### ### ###
#drop duplicates from high and low responder datasets
unique_high = high_responders[!rev(duplicated(rev(high_responders$Athlete_ID))),]
unique_low = low_responders[!rev(duplicated(rev(low_responders$Athlete_ID))),]
#get the percentage female in each dataset
(table(unique_high$gender)['W'] / sum(table(unique_high$gender))) * 100
(table(unique_low$gender)['W'] / sum(table(unique_low$gender))) * 100
#test for differences
prop.test(x = c(table(unique_high$gender)['W'], table(unique_low$gender)['W']), n = c(sum(table(unique_high$gender)), sum(table(unique_low$gender))))
### ### ###
#test for differences in the parkrun community component
mean(high_responders$parkrun_community_factor)
sd(high_responders$parkrun_community_factor)
mean(low_responders$parkrun_community_factor)
sd(low_responders$parkrun_community_factor)
t.test(high_responders$parkrun_community_factor, low_responders$parkrun_community_factor)
#test for differences in who participants came with
(table(high_responders$came_with2)['0'] / sum(table(high_responders$came_with2))) * 100
(table(low_responders$came_with2)['0'] / sum(table(high_responders$came_with2))) * 100
prop.test(x = c(table(high_responders$came_with2)['0'], table(low_responders$came_with2)['0']), n = c(sum(table(high_responders$came_with2)), sum(table(low_responders$came_with2))))
#test for differences in pre-run sociality
(table(high_responders$just_before2)['1'] / sum(table(high_responders$just_before2))) * 100
(table(low_responders$just_before2)['1'] / sum(table(low_responders$just_before2))) * 100
prop.test(x = c(table(high_responders$just_before2)['1'], table(low_responders$just_before2)['1']), n = c(sum(table(high_responders$just_before2)), sum(table(low_responders$just_before2))))
#test for differences in felt energy
mean(high_responders$how_energising)
sd(high_responders$how_energising)
mean(low_responders$how_energising, na.rm = TRUE)
sd(low_responders$how_energising, na.rm = TRUE)
t.test(high_responders$how_energising, low_responders$how_energising)
#test for differences in fatigue
mean(high_responders$fatigued)
sd(high_responders$fatigued)
mean(low_responders$fatigued)
sd(low_responders$fatigued)
t.test(high_responders$fatigued, low_responders$fatigued)
#test for differences in 5 km run times
mean(high_responders$time)
sd(high_responders$time)
mean(low_responders$time)
sd(low_responders$time)
t.test(high_responders$time.lg, low_responders$time.lg)
################################################################################################################################################
### MULTILEVEL MODELS ON RPE ###
#test the effects of each of the social predictor variables on RPE
RPE_1 = lmer(exertion ~ 1 + parkrun_community_factor + (1 | Athlete_ID), data = mydata)
summary(RPE_1)
r.squaredGLMM(RPE_1)
RPE_2 = lmer(exertion ~ 1 + came_with2 + (1 + came_with2 | Athlete_ID), data = mydata)
summary(RPE_2)
r.squaredGLMM(RPE_2)
RPE_3 = lmer(exertion ~ 1 + just_before2 + (1 | Athlete_ID), data = mydata)
summary(RPE_3)
r.squaredGLMM(RPE_3)
################################################################################################################################################
### MULTILEVEL MODELS ON FATIGUE LEVELS ###
library(lmerTest)
library(MuMIn)
#make binary predictors factors
mydata$came_with2 = as.factor(mydata$came_with2)
mydata$just_before2 = as.factor(mydata$just_before2)
### ### ###
#model for Hypothesis 1.1
hypothesis_1.1 = lmer(fatigued ~ 1 + parkrun_community_factor + (1 + parkrun_community_factor | Athlete_ID), data = mydata)
summary(hypothesis_1.1)
r.squaredGLMM(hypothesis_1.1)
#model for Hypothesis 1.2
hypothesis_1.2 = lmer(fatigued ~ 1 + came_with2 + (1 + came_with2 | Athlete_ID), data = mydata)
summary(hypothesis_1.2)
--r.squaredGLMM(hypothesis_1.2)
#model for Hypothesis 1.3
hypothesis_1.3 = lmer(fatigued ~ 1 + just_before2 + (1 | Athlete_ID), data = mydata)
summary(hypothesis_1.3)
r.squaredGLMM(hypothesis_1.3)
################################################################################################################################################
### MULTILEVEL MODELS ON ENERGY LEVELS ###
#model for Hypothesis 2.1
hypothesis_2.1 = lmer(how_energising ~ 1 + parkrun_community_factor + (1| Athlete_ID), data = mydata)
summary(hypothesis_2.1)
r.squaredGLMM(hypothesis_2.1)
#model for Hypothesis 2.2
hypothesis_2.2 = lmer(how_energising ~ 1 + came_with2 +  (1 | Athlete_ID), data = mydata)
summary(hypothesis_2.2)
r.squaredGLMM(hypothesis_2.2)
#model for Hypothesis 2.3
hypothesis_2.3 = lmer(how_energising ~ 1 + just_before2 + (1 + just_before2 | Athlete_ID), data = mydata)
summary(hypothesis_2.3)
r.squaredGLMM(hypothesis_2.3)
################################################################################################################################################
### MAIN MULTILEVEL MODELS ON ENJOYMENT LEVELS ###
#model for Hypothesis 3.1
hypothesis_3.1 = lmer(enjoyment ~ 1 + parkrun_community_factor + (1| Athlete_ID), data = mydata)
summary(hypothesis_3.1)
r.squaredGLMM(hypothesis_3.1)
#model for Hypothesis 3.2
hypothesis_3.2 = lmer(enjoyment ~ 1 + came_with2 + (1 | Athlete_ID), data = mydata)
summary(hypothesis_3.2)
r.squaredGLMM(hypothesis_3.2)
#model for Hypothesis 3.3
hypothesis_3.3 = lmer(enjoyment ~ 1 + just_before2 + (1 + just_before2 | Athlete_ID), data = mydata)
summary(hypothesis_3.3)
r.squaredGLMM(hypothesis_3.3)
################################################################################################################################################
### MAIN MULTILEVEL MODELS ON 5KM RUN TIMES ###
#model for Hypothesis 4.1
hypothesis_4.1 = lmer(time.lg ~ 1 + parkrun_community_factor + slowed_spedup_natural_3 + (1 + parkrun_community_factor | Athlete_ID), data = mydata)
summary(hypothesis_4.1)
r.squaredGLMM(hypothesis_4.1)
#model for Hypothesis 4.2
hypothesis_4.2 = lmer(time.lg ~ 1 + came_with2 + slowed_spedup_natural_3 + (1 + came_with2 | Athlete_ID), data = mydata)
summary(hypothesis_4.2)
r.squaredGLMM(hypothesis_4.2)
#model for Hypothesis 4.3
hypothesis_4.3 = lmer(time.lg ~ 1 + just_before2 + slowed_spedup_natural_3 + (1 + just_before2 | Athlete_ID), data = mydata)
summary(hypothesis_4.3)
r.squaredGLMM(hypothesis_4.3)
################################################################################################################################################
### MULTILEVEL MEDIATION MODELS ###
library(mediation)
set.seed(2014)
detach("package:lmerTest", unload=TRUE)
#make binary predictors integers (a requirement of the mediate package)
mydata$came_with2_int = as.integer(mydata$came_with2)
mydata$just_before2_int = as.integer(mydata$just_before2)
################################################################################################################################################
### HYPOTHESIS 5.1 ###
#mediation relationship: parkrun_community_factor -> how_energising -> time
med_fit_5.1 = lmer(how_energising ~ parkrun_community_factor + slowed_spedup_natural_3 + (1 + parkrun_community_factor | Athlete_ID), data = mydata)
summary(med_fit_5.1)
r.squaredGLMM(med_fit_5.1)
out_fit_5.1 = lmer(time.lg ~ how_energising + parkrun_community_factor + slowed_spedup_natural_3 + (1 + parkrun_community_factor | Athlete_ID), data = mydata)
summary(out_fit_5.1)
r.squaredGLMM(out_fit_5.1)
med_out_5.1 = mediate(med_fit_5.1, out_fit_5.1, treat = "parkrun_community_factor", mediator = "how_energising", covariates = "slowed_spedup_natural_3", sims = 1000, boot.ci.type = "bca")
summary(med_out_5.1)
################################################################################################################################################
### HYPOTHESIS 5.2 ###
#mediation relationship: came_with2 -> how_energising -> time
#more complex random effects structures failed to converge
med_fit_5.2 = lmer(how_energising ~ came_with2_int + slowed_spedup_natural_3 + (1 | Athlete_ID), data = mydata)
summary(med_fit_5.2)
r.squaredGLMM(med_fit_5.2)
out_fit_5.2 = lmer(time.lg ~ how_energising + came_with2_int + slowed_spedup_natural_3 + (1 + came_with2_int| Athlete_ID), data = mydata) # more complex random effects structures don't converge
summary(out_fit_5.2)
r.squaredGLMM(out_fit_5.2)
med_out_5.2 = mediate(med_fit_5.2, out_fit_5.2, treat = "came_with2_int", mediator = "how_energising", covariates = "slowed_spedup_natural_3", sims = 1000, boot.ci.type = "bca")
summary(med_out_5.2)
################################################################################################################################################
### HYPOTHESIS 5.3 ###
#mediation relationship: just_before2 -> how_energising -> time
med_fit_5.3 = lmer(how_energising ~ just_before2_int + slowed_spedup_natural_3 +  (1 + just_before2_int | Athlete_ID), data = mydata)
summary(med_fit_5.3)
r.squaredGLMM(med_fit_5.3)
out_fit_5.3 = lmer(time.lg ~ how_energising + just_before2_int + slowed_spedup_natural_3 + (1 + just_before2_int | Athlete_ID), data = mydata)
summary(out_fit_5.3)
r.squaredGLMM(out_fit_5.3)
med_out_5.3 <- mediate(med_fit_5.3, out_fit_5.3, treat = "just_before2_int", mediator = "how_energising", covariates = "slowed_spedup_natural_3", sims = 1000, boot.ci.type = "bca")
summary(med_out_5.3)
################################################################################################################################################
### HYPOTHESIS 6.1 ###
#mediation relationship: parkrun_community_factor -> fatigued -> time
med_fit_6.1 = lmer(fatigued ~ parkrun_community_factor + slowed_spedup_natural_3 + (1 + parkrun_community_factor | Athlete_ID), data = mydata)
summary(med_fit_6.1)
r.squaredGLMM(med_fit_6.1)
out_fit_6.1 = lmer(time.lg ~ fatigued + parkrun_community_factor + slowed_spedup_natural_3 + (1 + parkrun_community_factor | Athlete_ID), data = mydata)
summary(out_fit_6.1)
r.squaredGLMM(out_fit_6.1)
med_out_6.1 <- mediate(med_fit_6.1, out_fit_6.1, treat = "parkrun_community_factor", mediator = "fatigued", covariates = "slowed_spedup_natural_3", sims = 1000, boot.ci.type = "bca")
summary(med_out_6.1)
################################################################################################################################################
### HYPOTHESIS 6.2 ###
#mediation relationship: came_with2 -> fatigue -> time
med_fit_6.2 = lmer(fatigued ~ came_with2_int + slowed_spedup_natural_3 + (1 + came_with2_int | Athlete_ID), data = mydata)
summary(med_fit_6.2)
r.squaredGLMM(med_fit_6.2)
out_fit_6.2 = lmer(time.lg ~ fatigued + came_with2_int + slowed_spedup_natural_3 + (1 + came_with2_int| Athlete_ID), data = mydata) # more complex random effects structures don't converge
summary(out_fit_6.2)
r.squaredGLMM(out_fit_6.2)
med_out_6.2 <- mediate(med_fit_6.2, out_fit_6.2, treat = "came_with2_int", mediator = "fatigued", covariates = "slowed_spedup_natural_3", sims = 1000, boot.ci.type = "bca")
